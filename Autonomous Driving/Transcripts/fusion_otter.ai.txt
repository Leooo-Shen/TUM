Unknown Speaker  0:05  
The task in what is a data fusion is to arrive at a robust perception. Robust perception is key for autonomous vehicles and humans. If you look at the skirt, the skirt had many people in an uproar because many people thought it's white and yellow, or white and gold, and many people thought it's blue and black. If you look at the left photo that was taken and it spiked the discussion, many of you disagree if you look at the actual product to the right, which I bought, just to be sure, it's easier to arrive at an opinion. That is a wet good estimate of reality. Humans disagree sensors to us well, if you develop an autonomous vehicle, you will have mismatching information for varying sensors, we will have false positives and false negatives. So use multi sensor data fusion strategies in order to achieve the best estimate for your task. It is obvious that if we have false positives and false negatives, if we have uncertainties, we want to improve our perception we want to improve our fusion. What are some key performance indicators we can use in order to assess if we have two versions of the algorithm which one is more useful to us? A well liked method is the using the receiver operating characteristics curve to see through which we can represent the challenge of autonomous driving in a single image. The top left promises 100% true positive, while the bottom right promises 0% true positives and 100% false positives, you want to move your algorithm in the top left corner. By selecting better algorithms, you can move the curve to the top left. By selecting different kinds of thresholds you can move along your curve, but you can typically not reach the top left curve in practice, which means that monitoring is required in order to ensure that your system is safe and performant. Even in situations when you do have false positives and false negatives. Automated cars do require precise self localization. They need to know where they are, and the localization of obstacles around them of lane markers to evade obstacles to break in time to follow the road. To do this, they need sensors, many sensors available dominantly laser sensors, colliders, radars, cameras, ultrasonic sensors, GPS, rain sensors, thermal imaging sensors. Ideally, you should become able to make your own proposals for improving and designing sensor sets for autonomous vehicles. Where should you put the different kinds of sensors, which different kinds of sensors for to use, which parameters of sensors are relevant to you when you select them. It is often helpful to sort sensors into categories of groups where they can perform certain tasks for which they are especially well suited. One way of doing this is to sort them into passive and interactive sensors, where the passive sensors are those sensors that do not actively emit a signal and just re measure reflections from other sources. For example, cameras or electric field sensing devices or acceleration measurement devices. On the other hand, active sensors which actively emit signals from the sensor that can then be reflected or scattered by the environment back to the sensor, such as radar or LIDAR, so ultrasonic sensors or GPS. You can of course, think about grey areas when you have a smart camera that targets the car lights to serve position. But still, it's useful to think about the properties of your sensors and sell them to the tastic. And another option is to sort the sensors into those sensors, which are imaging sensors and those sensors, which are not imaging sensors, simply saying, is it a sensor that creates an image or is it a sensor that creates a point cloud, just pointing out that for example, the camera would often then be categorised into the imaging category, and the radar would be categorised into the non imaging category. However, depending on how much money you invest into the purchase of the radar, or how much effort into the development of technology, you can also create images from highly advanced Raiders. And they are also white papers from diamond for example, that give you recommendations about how you would go about improving radars further in order to best fulfil the desires from the cars. So again, a definition with grey zones but also again a definition which is useful.

Unknown Speaker  4:49  
So a few slides ago I asked you which parameters of a sensor would be relevant for you if you were to select the components for your sensor set and let's take a look One. So maybe we could study the angular resolution of lighthouse and see what the actual impact is. You can think about it yourself, if you have a bigger angle between each individual lighter beam, what impact does that have on your returning point cloud? And how does this kale with the scenario that you're observing. So here, you have the four scenarios where you are following a police car. And you can see the point cloud that is returned in the top of detail. And you see that when the car is very close, you get a very nice point of reflection, which makes it much easier for you to create a polygon box, please note that you're still missing the occluded areas, which you would have to fill otherwise, if you need them. And as the car now travels further away, and as it changes, it's relevant relative position to us with regards to which lane it is on, we lose additional areas of vehicle detection. And as the car gets further and further away, we have less and less points on it, where if we go far away, we can have very little points on the car itself, actually. Now, if you were to, if you were to just compare this to some noise points, we can just draw the noise points here. Or like points from a small obstacle. How do you differentiate those from the reflections that you get from the count. So you also sometimes have to work with little extra data. And by combining measurements over time, by applying plausible isation, by combining multiple sensors, by enhancing your sensors, and so on, by exploring your topology with all the information that you have, you need to come up with systems that provide sufficient performance such that functions that then uses data such as automated braking systems, autonomous driving systems, actually can deliver sufficient performance. And if obstacles become small enough, they can actually fit between your individual attributes just as an example. And then maybe do not get detected in one individual ladder measurement. So this is just one example for a parameter that has impact for one sensor type. And we will see is also coming back to the ways to structure the sensors into different categories. Let's take a look at a more data science oriented approach. We can have features that we can derive from different kinds of sensors. And then say this is the information we extract it can then be used by follow up functions, we can talk about location that is provided by sensors to ask different obstacles in front of us, which we can draw from the stereo cameras from the 360 degree laser, in this image for the radar sensor from the ultrasonic sensor. But we can only draw it from the GPS, if we have connectivity so that the other car could tell us that it is in front of us based on its GPS, we can talk about range sensors to arrive at a similar picture, we can talk about a range rate to say is the target moving closer to us as moving away, we can talk about the size of objects is about I can talk about extended obstacles, which is very important. If you're trying to decide if you're going to collide with an obstacle or not. Then we can talk about classification itself to say, Is this just an obstacle? Or is this actually a car? Was this a truck? Or is this a bike very often thresholds are used to differentiate between detection and non detection, you can have a gigantic neural network and then in the end of our chain, you still find a very simple threshold that determines the outcome. And this has upsides and downsides. What you can typically do is have a signal the signal has some laws, and you will say that it is a certain peak in a signal that probably represents an reflection from an obstacle that goes beyond the signal to noise ratio. And

Unknown Speaker  9:08  
the challenge is of course, which vessel Do you select? If you have non adaptive sensing thresholds, then often they only apply to certain scenario or they they don't scale with the ageing of the vehicle. So sensor calibration and agribon. Calibration again is a very important topic. There have been accidents of autonomous vehicles in this case are thinking mostly of autonomous test vehicles for the examples where we try humanity try to the engineers try to decide for certain vehicles that they were proceeding with the sensors was actually moving or stationary. If an obstacle is stationary, for example, a wall you don't expect it to change its position so you might be driving more confidently with your autonomous vehicle if you pass it. If you're observing an act car being driven by human that is currently moving, you might expect this obstacle to move in your way. So you may be more careful, you can have small children around you that you can maybe expect to be more spontaneous about motion decisions. So you have to keep keep even bigger distances. And there are many, many special scenarios. And often this these seemingly trivial distinctions, for example, saying is an object stationary or moving turned out to be not so simple if you want to scale into a huge number of scenarios based on sensors that are available. There's another example of autonomous test vehicles, you're one of the challenges that we're undoing the data that was driving along the road, and they had made a simple threshold algorithm to decide if they were driving towards the road, or if they were being bounded by the pedestrian walkway. And when there was a pipeline going across the road, the car actually stopped, because it thought that the small pipe for the rain that went across the road was actually a capstone implicating, pointing towards the pedestrian walkway. and due to this, the car had to be rescued, again by the human operators. The key point here is that first roads are actually an important topic, how do you select them? How do you how do you adapt them? What are the best ways to express your thoughts. So you will run into this topic when you're working autonomously because this is the example with the rain get I was mistaken for a curbstone. So you see the perception of the vehicle. In the big image, you see the characteristic lines of lighter, that get wider and wider as you go further away from the vehicle and that have this blind spot close to the vehicle because in this case, the lighter is on top of the vehicle. And you have forward facing camera, which is looking at this intersection here. And you see that the arrows here indicate an obstacle that is right in front of the vehicle. And this obstacle actually stopped the vehicle, you can find these relevant data situations in the grand challenge videos that you can also find on YouTube. In order to deal with your false positives or false negatives, your uncertainties, you have to make certain assumptions and exploit them. Such assumptions can be the idea that the vehicle drives on the road or that the world is flat or that the other vehicles also drive on the road or that vehicles have a certain size or certain maximums leads to accelerations, many more possible assumptions exist. And it is a common strategy to agree on a set of assumptions that is as small as possible. And from that set of assumptions, try to come up with a formally

Unknown Speaker  13:09  
proven

Unknown Speaker  13:11  
strategy. That way you can guarantee that safety is present. And here you have just one example where you might have your assumption of the vehicles driving on the road to be violated. And now of course, if in your algorithms for your perception or mot sensor data fusion, you would have the system that automatically eliminates all flying cars, then the question is what would happen if you were to drive on the lane this week, partially occupied. So if you read to be coming in this direction, would you actually drive into the vehicle even though your senses perceive it. And similar accidents have happened in the past. Let's take another look at data sheets of certain sensor type together. This is a LIDAR that we're looking at. And you can see that it comes with a certain wavelength with which it emits the signal that has an impact on how well it goes through the rain. And however it is reflected, you see how it is actually measuring its returns. So in this case of time of flight approach, you see the range range is very important. But it is also important for you to understand that while it says 200 metre range, it says this is for an average target. So now maybe you can think for what just one second, what is an average ladder target. And an average ladder target probably is referring to a certain reflectivity to a certain size, the certain orientation that you do orientation you also have here similar to preview direction, which means that as you're putting your LIDAR into your car, maybe at the front and you have a certain field of view as obstacles traverse through this field of view, or as you rotate your car and so on detections may or may not occur depending on the combination of all of these parameters, which leads to relatively sophisticated sensing challenge, you have the exact fields of view, where to the greatest laser even scan for obstacles, you have to see the horizontal and vertical direction, you basically have the car from the sides, and you can have the vertical field of view, while at the same time, you can also have the horizontal field of view. And you have specific specialties of the lighter For example, here moochie Echo, which determines if you have a certain signal, where we also had this discussion already with the detecting thresholds. You would, for example, exploit only one of these specialists or maybe return three of these specials or more, as you can see here, this can help you with if the first detection for example, comes from false positives, like the rain Did you don't cut away the follow up detections that you might have? Or if you hit things through glass windows, for example, you have the question of the data update rate? How often do you actually get measurements This is important because the obstacles around you to move. So whenever you have to select the frequencies that are acceptable for you, or define requirements for it, you have to ask yourself, for example, here within 25 words, how how far do the obstacles around me actually move? And is this distance that they move while not observing them? Is it small enough such that you can tolerate this? You do have the question of the operating temperature. This is often also defined as industrialization. If you have all the nice robotics properties fulfilled, it's still not clear if you can actually use the sensor within the car. Is it robust enough? Is it compliant with requirements? Does it not emit too much electrical noise to not disturb any systems around it. Then again, we have the accuracy, which you study again for your vehicle. So this now relates to your uncertainty. When you build up all the sensor data fusion strategies, you always have the challenge that you have very nice mathematical models. But you have a hard time coming to the correct parameter estimates that you need to have in order to use those for your estimations. You have the resolution that we talked about in different angles and also distance. And then you have more system architecture related aspects. So for example, the question how much data bandwidth you're actually using. And you have properties also about

Unknown Speaker  18:05  
functions that the sensor comes with. So a sensor can only provide you with the raw data, for example. So you only get the point cloud that was detected role. Or as you can see here, the sensor on its own can also provide certain higher level output, like tract objects. In this case, you don't just get a point cloud you, for example, could also get a vehicle detection, where the vehicle in this case might be provided with a position with a size with the speed. And you see here that in this case, you only get this at 6.25 hertz. So if you set up here, 25 hertz is fast enough for you. But then you want to use the trick is now you only have 6.25 hertz. So you again have to consider how does all of this fit together, you have to either motion compensation, which in this case means that you can provide the sensor with certain information of your car, probably the twist information, the velocity of the car, the rotation rate of the car, and you get this in this case, I have the Canada face. So you need to make sure that you can provide this data to the sensor, and that you have access to improved object tracking. So many details. And this is just a small lens at one individual sensor. So these things keep people busy a lot when they are observing sensor sets for normal speakers. As we said, certain sensors have certain signature patterns, you can again see here the distance between the different LIDAR scans that you have. There is state of the art idea to accumulate frames from multiple measurements. And then you add them together you correct all of them for the emotion declared in between. So you get more scans across the whole road to get a more refined representation of it. Similar to as with emotion, you cannot really work with perception audit? Well, if you have zero understanding of the physics that dominate the perception itself or the sensors, and it helps to have some realistic ideas about how all of this, all of this stuff works, this is not a physics class. So the expectation is that most of you should have already studied each one of these effects in school. So we only touch on them very, very briefly. And you can check them out in more detail if you like that knowledge previously. So now we are emitting signals and we are interested in how do they travel through the world and how do they return to us eventually. And refraction is simply a wave crossing from one medium to another, experiencing a change in direction while continuing to travel to the new medium. And you can see this at the left image where you have the line of the light, going a little bit further to the bottom of the image. The relations in the angles between the different beams of light that you can see in the image should also be known to you from previous education. Otherwise, feel free to check them out in more detail. So you can calculate how the entry angle relates to the exit angle, both for reflection and refraction. The effects of diffuse or specular reflections are important because they both have upsides and downsides for our ability to detect an obstacle. The question if we get a diffuse specular reflection depends on the surface roughness relative to the signal wavelength, as you can see indicated in this image below here. The specular reflection is one way you get a sharp reflection with concentrated energy of coming back to you. So think of a mirror for example, that gives you all

Unknown Speaker  21:53  
your liner that back,

Unknown Speaker  21:54  
which would make for a long distance possibility of detection. But at the same time, if you angle the mirror such that all the lighter light that hits the mirror is then reflected away from you and you never see it again with your sensor. This can also be leading to an unfortunate false negative if you then miss the mirror itself. This is not stable for the individual situation but also depends on the exact parameters of the situation. For example, rain falling on the road can change the surface roughness can change the way it reflects and then lead to specular reflections. Simply a case where this is annoying is when you use the lighter in order to detect lay markers on the road. But the whole system is designed in such a way that if it's wet, the laser beam is reflected away and doesn't return to you. As you can take a look at the specular reflections over on the right side. scattering refers to the effect where light is being forced to deviate from a straight path due to localised non uniformity in propagation medium. This can happen due to droplets think about rain or fog, or due to surface roughness. And you can see some illustration on the right. When absorption comes into play with targets that you're trying to detect, that can be very unfortunate, because in this case, you're shooting your sensor signal at the obstacle and then the obstacle is actually absorbing the wave not giving it back to you. As you can see in the bottom right image here with the anti noise chamber This can also be a desired effect or some construction or sensors if you want to limit radiation from the sensor can actually go. And if you want to have a fun read, you can also acquire books about stealth fighters and how they are built and also counter strategies to still detect them. In in theory, you can also try to detect that your signal was actually absorbed by instead of looking for reflection looking for the missing reflection. And in general in autonomous driving typically up for tags that do not have such a high absorption when you want to detect them. Now when we lose our signal, why we are travelling through space to our target, and we combine the effects of scattering and absorption, and everything that makes up our signal loss we can be talking about attenuation. If we're talking about transmission, this means that a wave that we have emitted propagates and crosses from one medium into another and is transmitted through the medium successfully. and if we talk about diffraction, we're talking about the change of direction intensities of waves passing an obstacle or an aperture with size approximately to the wavelength of the waves. So this concludes our brief reflection Have these sensor physics effects. Again, if you had zero information about this, then please check it out in more depth on your own. We expected, the majority of students already have heard of these

Unknown Speaker  25:14  
effects. Let's take a look at some questions that can help you guide your exploration of sensor set. So imagine that you are tasked with designing the sensor set for new autonomous vehicle or new autonomous robot.

Unknown Speaker  25:28  
The first question may be, what do you actually want to measure? And why? Of course, it's obvious to say we want to measure everything that is around us and perfect quality. And then if we have everything around us in perfect quality

Unknown Speaker  25:43  
at minimum latency, then of course, we can do everything with this. So this is obviously the best thing, so why not go for it? And the answer is that unfortunately, the higher up your brain the requirements for your perception, the higher the sensor costs go, the higher the development costs go, the more computational power is required. And then still there are physical limitations that you may not cross. So you will always end up with an imperfect representation in any case. So it is really meaningful to understand what are you going to do with the information? If you understand the requirements of the function that is going to use the output of the witnesses the data fusion of the perception, then it is easier for you to also say, at which range? Do you need to detect certain things? What kind of obstacles Do you need to detect? Now after you've made a selection of the obstacles that you want to detect, or the environment that you want to detect, or the information that you want to acquire? The next question is, how do you want to acquire it? So what's the best way to measure this? Again, cost comes into play, you're typically looking for performance, but cost efficient ways of measuring the things that you want to measure. And it should also be meaningful. So if somebody asks you, why do you use the radar detector, you should have a meaningful response to this question. And you should also be aware of the alternatives so that you can make a good choice. Now, let's say you put together this measurement, set of sentences. So you put together a set of sensors so that you can actually perform measurements. And the question is, if you measure something, was it a success or not? So let's say you measure a vehicle ahead of you at 70 metres of distance. Now, how do you know if this was good enough or not? So which metrics Do you plan to apply? In order to say if you need to further improve your measurement? Or if it's already sufficient? And how do you determine the accuracy of your measurements? If you're talking about safety, if you're talking about reliability about robustness, mechanisms, such as redundancies come into play, so it is a good question to ask if the things that you decided you need to measure and that you're not measuring, and they're also evaluating if your KPIs if if you have redundancy in your measurement principles to cover them, such that when one measurement principle is weakened, for example, you're using a camera without light to detect something during the day and then becomes night now use camera. Can you know you know, I also use the radar.

Unknown Speaker  28:20  
So how is your your system combined? It is very good to document the reasoning of these things,

Unknown Speaker  28:26  
and to document also the results of simulation analysis measurements and so on. Because, in the end, often, if they have a system of safety critical, somebody will ask you, but how do you know all this and then if you say, well, we tried something in the past, but we don't quite remember what we did. This is not good enough for any expert review. So documentation here is part of what you need to do or what you must do in order to bring these systems into the market.

Unknown Speaker  28:53  
Typically,

Unknown Speaker  28:55  
you need to validate and verify your assessments. So all the IDS DSD came up with, it's not guaranteed that they actually correct. So you need to if you if you need to run certain tests, then you have to do that. In generally, this is an opinion that many people share, we're experts in the autonomous driving domain. That is that developing an autonomous vehicle fully from scratch at your desk, where the vehicle is supposed to scale to lots of situations, and to be very robust with many users and so on is pretty much impossible. So you you have to bring your ideas on the road, you have to bring these on the test track and see if they actually correct and sometimes you also need to perform a certain amount of research performance at amount of tests in order to make intelligent design choices. So maybe get two sensors compare both of them to see which one is performing better and so on. We can also ask if we have the correct requirements for all sensors identified. And then on the one hand, you want to correct requirements, but then on the other And you also want to know if the sensors work as they're supposed to. So even if your data sheet says that the sensor does have a range of 200 metres, you actually do need to verify it yourself. You can, of course blindly rely on the information, so choose. But then what if, in the end, you build everything together, and the very last minute, you notice, that sounds like actually only needs one out of 50 metres of effective range for your scenarios. So performing some tests on your own is also very valuable. And then again, if we have all this approach together the questions of course, do we fulfil the norms that we need for certification if we need to fulfil certain safety levels if we need to perform certain quality levels? Have we actually checked the norms if we check the requirements for the certification? who is doing the certification? discussions should be answered? While doing the design for instances? Typically, multiple sensors of the same type are available for your task. So the question is, how do you compare the different sensors is just one example for what graders multiply does. Not saying that you should choose these but just show you some examples for the different so you're familiar with it. See here, one sensor has 200 metres, it's described as for an average target, to give another sensor that has 90 metres This is described for 90% readmission target. So you sometimes also have incompatibilities in comparison between the different data sheets, so you have to standardise it to one piece type of information that you can handle, you see that the ranges vary from 40 metres to 200 metres. So perhaps the 240 metres would be for a city or for for robots. It's quite short actually, for cars, where the 200 metres is already highway distances, which is very nice. And then you have 10 metres here, this sounds even more like robot suitable laser. But many applications exists where you can could use them in different applications,

Unknown Speaker  32:05  
you

Unknown Speaker  32:07  
if the same situation for Raiders. And quite often in cars, actually, you did use a combination of long range and short range Raiders. And then you just combine the outputs from those in your recent data fusion. In any case, you have to run through these comparisons when to creating your sensor set. We already said that the target properties impact the effective detector range of our sensors. And here's an example for two lighters. And we see for example, that with a target readmission of 20%, we need to be at least 35 metres close to detected with the HR ladder. And we need to be at least 55 metres close to detect it with the as our ladder. So the bigger better or target initial, the further away, we can still detect the target. Again, we said that the assumptions that we make are very important. So for example, with the lighter beam, we can ask ourselves, is it actually.or? Is it not? So if you're looking at a road, and there's a traffic sign, and you're asking yourself, is our ladder been going above it, or is our ladder beam going to hit it, it's quite important to ask if the ladder beam is a perfect.or. If this is now already within the opening angle of the traffic sign, and also, if there are multiple obstacles, in case it would be adopted would only hit the second obstacle here. In case it is as an opening angle. It will detect what collide with both obstacles. And you can hear some examples of how it actually increases in size and an angle over time. And this can be significant. weather impact is important. You have here a separate illustration of snow when the left image is very easy. Even if there's rain to say where is the road on the right hand side. It's really difficult to tell if humans are walking on the road or not. And you can of course ask does the autonomous vehicle have to work in both situations and this is not a black and white answer. But certainly the availability of autonomous vehicles is dominated by the ability to deal with noise and disruptions. So vehicles do not have to be able to drive during an apocalypse. Meteor intact that is a huge flood which is a use case that is typically ruled out. However if the smallest snowflake is already disabling your car, then you're in the same situation as termed racism. And you know what to say about that. So even for the trains, we should try to improve the situation. In any case, it is actually a joke about snowflakes, where two snowflakes meet in the area to get in one of the snowflakes says to the other snowflake, hey, do you want to go to the city to disrupt the train system, and then the other snowflake says, I know it's so necessary, you can do that load. So in any case, making autonomous vehicles more robust towards weather impacts is always beneficial. sensors can also react very differently under certain lighting or weather conditions. As you can see, for example, the glaring within the lights become precise, the areas around the car turn red droplets on the screen, impact the visibility, and you have certain processes and mechanisms to try to get rid of this, you have certain sensors that are more or less vulnerable to these situations, you just have to consider them. We briefly touched on the vehicle technology for lighters, for example, also exists for our sensors. To give you cleaner example of that. Here's an image from an accordion data sheet, we see that you have multiple thresholds. And each of them represents different things. So first, you have here in this test scenario, a strong reflection from a glass window, then you have a much weaker reflection from a raindrop, then you have an area where this is just considered noise. And then you have an area where you have again a big reflection from a vehicle that is detected through the glass window and and now you need to know which one of these peaks does your sensor actually provide back to you at the interface.

Unknown Speaker  36:58  
A nice experiment was done in the article where two LIDAR systems were used with similar but slightly different wavelengths, and slightly different beam divergences. And you also already see that theoretical range the design is then proclaimed accuracy in operating temperatures, obviously, here you have ladders that can work on the minus temperatures. And they wanted to see what's actually the impact of snow on these two lighters that were studied in 600, you can see the names of here. So you have example with target at 11 metres in front of the lighter, you have got moderate snow going on. And you see that the target is here at 11 metres, but detections you're actually getting from 10.6 to 11.5 metres. And they noticed that the standard deviation was increased by the snow. Then they compare one example with moderate snow in one example with heavy snow with a target at four metre distance. So you see that you have here for the moderate snow on the left side, a strong peak at four metres were for the heavy snow, you have nothing at four metres. So all of this is supposedly snow reflections. And you could theoretically drive through this now, however you have to be sure is this an obstacle that you can drive through not drive through is not an obstacle. So just know that you can actually drive through. And you can ask yourself how easy or difficult it is to tell this only based on this information. For both sensors, you're also getting infinite signals, ie in this case where they didn't get a return at all. And also for the moderate snow situation, they can get some early detections as well. What you have to ask yourself is is this relevant to the driving function that you're trying to realise then you have to solve the problem? Or is it not then you can make it art. Thank you very much for your attention, and we look forward to covering the multi sensor data fusion algorithms with you after we finished the sensor physics investigations. Thanks and bye

Transcribed by https://otter.ai
