Unknown Speaker  0:06  
Welcome back to the autonomous driving lecture series. We will now continue with our journey through the robinus architecture that we are studying as an example for an eight us and AV architecture. As you remember, the goal is to make you familiar with a number of modules that are relevant for such a vehicle. As well as with the general idea of architecture design, which is driving idea in computer science in general. After we conclude this, we will take another look at possible modules, but from an algorithmic perspective, where right now we are looking at it from an architectural and interface perspective, as well as from a general problem introduction perspective. After discussing the sensor systems and basic perception data models Last time, we will now venture into the multi sensor data fusion domain. Again, we will return to this from an algorithmic perspective. But for now try to grasp the general problems and the responsibility of the different modules. The background for our data fusion issue are the imperfections of our sensor perception. If all detections would be perfect, the technology could be so much easier. However, all of our sensor detections are subject to imperfections and errors. many examples can be discussed, however, for today, let's take a brief look at an example for RGB D cameras, where RGB D stands for red, green, blue depth, and depth implying distance from the camera to the corresponding image coordinate. There are many ways to derive depth information. But on this slide, we are mostly interested in errors that depth image could contain. You can see in the image, a strong reflection on the ground caused by a lamp

Unknown Speaker  1:55  
and the corresponding depth image, you can see a black hole. If you were a robot, you could well think that this is a hole to fall into. Where as a human, you would like to think that this is a lamp reflection within a perfectly sound floor. The flaw detection is missing in these positions, which you could consider a false negative, no detection is present where the detection of the flaw would be desired. You can find a lot more information about this specific effect and the intertext linked as a reference at the bottom right of the slide

Unknown Speaker  2:36  
which considers the use of certain optical filters to improve these insufficiencies. You can see an application of such optical filters in this case, linear fin polarisers at the bottom left of the slide. And in the image to the right, you can see the improved perception derived with the available filter. However, since modifications typically come with upsides and downsides, trade offs are a common thing in autonomy design, limitations can be overcome at varying costs. If you find yourself in a similar design situation. In the future, consider the limitations you're willing to accept and which limitations you want to improve with edit software efforts or hardware costs. After looking at the false negative example, let's take a look at the false positive measurement. In the new image to the right you can see the reflection of objects in the background causing the false positive perception of an obstacle closer to the robot. And similarly again an improvement with the food application.

Unknown Speaker  3:55  
Perception imperfections functional insufficiency is false positives, false negatives, in accuracies this will always remain and be a part of your data. This is where multi sensor data fusion comes into play as it tries to provide the best estimate of the truth based on the sensor measurements you provide based on the sensor models you use to describe your sensors and based on the context that you allow your system to use in its estimation. To achieve this, both modelled and learned approaches exist and each have their own merit and we will come back to this in the algorithmic part. Return capability deep learning based approaches for multi sensor data fusion and tracking are still relatively recent, but have shown improving performance in the last years. Object fusion can be meant understood that we are combining the output of the sensor detection in such a way that the output of the fusion system are actual objects such as vehicles with the previously described properties. How if you compare this to grid fusion This could be understood to mean that the output of the fusion system would be a grid map, for example, an obstacle grid map which would show the cells around the vehicle which are occupied. Let's take a closer look at these different fusion approaches. Object fusion often provides you with point coordinates or 2d or 3d bounding boxes to represent objects including but not limited to, vehicles, pedestrians, cyclists or road signs. These are relatively easy to describe with boxes, whereas the other objects such as roadside boundaries, or curbs, are hard to describe with boxes in a meaningful way. I provided you with two references in case you're eager to apply some object detections yourself right now, feel free to pause the video and check out the references. If you'd like standard object detection software is available off the shelf, and can be applied in a very short time as long as you're not immediately aiming for the best, most robust detection results. Which of course, if you're working on autonomous driving, you are aiming for the best and most robust detection results possible.

Unknown Speaker  6:13  
Besides detecting objects in individual images, a common challenge is tracking moving objects from frame to frame, typically associating the detected objects with unique tracking IDs. In the image, you can see an example of pedestrians being dragged while moving through a pedestrian zone. You see bounding boxes around the heads with dragger IDs next to them and you see lines behind them representing the travel paths. Ideally, if an object would leave the sensor field of view, and return data, you would like to recognise it and associated with the same original unique tracking ID. To simplify the problem of tracking unique IDs, Robin was proposed to set a new object property to true when the new object was detected. Just detecting an object is one thing assigning the correct classification to it is another challenge. If you look at Tesla videos on YouTube, you will find many situations in which cars are classified as trucks and vice versa. However, this is not a specific Tesla error, but a general challenge for computer vision experts. Examples for such classifications could be as mentioned before, vehicles, pedestrians, cyclists, or road signs. And once you have specifications, you can use it to shape your fusion and tracking algorithms. For example, when predicting a position change from frame to frame, you would have different expectations depending on the object being a human or vehicle. If you're uncertain which classification is correct, you could make multiple hypothesis and progress each one for the following measurements until you can falsify all that one hypothesis. Of course, theoretically, or practically, you could end up with a situation where none of your previous hypothesis makes sense anymore, which could have been the possible time to reset your current models and start from scratch with the obvious upsides and downsides. But we will get back to this. Instead of working with objects or working with objects exclusively, you could look at grid map approaches as a complimentary method. Converting sensor measurements into occupancy grid maps as one example, a grid map can be imagined as a matrix of rectangular cells surrounding the ego column where each grid map cell could be filled with all kinds of semantic information. For example, with occupancy descriptions, occupancy grid maps set the state of a cell to occupied or unoccupied or unknown depending on your sensor models, detections and their geometric relations to the cell locations. For example, if you would shoot laser beam at a target, to detect it, mind you not to destroy it, we're not talking about those kinds of laser beams. Now if the laser would travel from your sensor straight to the detected object, you might assume that the cells through which the laser beam travelled, which did not cause the detection could be free. So we have the laser, we have the laser beam, which travels all the way to some kind of object, and then it returns. Now you have the detection over here to the right side. And you would assume that all the way in the middle would be free because if there would have been an object here, then we would have assumed our laser to just travel through to the object and then come back from them. However, just to point out some of the complexities, how would you have marked your grid cells if the laser beam just never came back and was reflected or absorbed somewhere else, or if it was first reflected somewhere else by mirror, and then ultimately came back from a different direction. We cannot just fuse objects like vehicles we can also fuse our ideas of where roads are And gold, often known as road or lane fusion. Often lane models are used to build a hypothesis and match them with sensor data. Similar to how you would previously MAP sensor data to your object models, like moving cars or pedestrians. Some companies, depending on their individual business case, use high resolution naps in order to improve lane detection, fusion and localization. Often hierarchical moody hypothesis approaches are used to deal with the challenge. The illustrations below show you how it is done. In the case of Robin OS, you have the top level road network topology which contains metre connections between longer road segments, which could be interesting for navigation. For example, if you if you want to switch to this other otium then you have the top level road network geometry, which gives you a better description of the actual shape of the road, which could be useful if if you want to know when to expect curves.

Unknown Speaker  11:13  
You have the low level road network topology, which could help you plan lane changes. The moody hypothesis approach is illustrated in this image showing either merging behaviour or oil scenario with two lanes depending on the detection of a construction site on the right side lane or free road. Finally, you can see the low level road network geometry, which could for example be useful to help you with centering yourself inside of your of your lane. With this we conclude our current exploration of sensors detection and fusion and journey on into the realm of decision making in motion. The models we talked about now use the output of the previous fusion modules, respecting the uncertainties of the provided measurements and try to make the best plans and decisions based on these uncertain inputs. Taking the uncertainties of the actuators into concern as well. Yes, not only the sensors are imperfect, but the actuators are as well. If you tell your vehicle to move according to a certain plan, you will still have to control the outcome and plan with some limited motion errors. This is easy to understand if you imagine driving faster and faster on a road entering unstable driving conditions on the racetrack. At some point slipping off the road even though you were steering into the direction of the lane. As we have seen before different robot functions could have different conflicting interests, which cannot be executed at the same time, such as braking and accelerating at the same time, which would make any sense robinus uses situated behaviour arbitration blocks to harmonise these different desires of the varying robot functions. Beware that alternative approaches exist as well and we look at this as an example. The task of the state of behaviour arbitration process or rather the individual sub modules within it is to analyse the given road situation to compute a desired path or action. And then the overarching situated behaviour which ration block has to decide which of these suggested paths or actions should be executed based on the results of this situation analysis. This iterative behaviour arbitration approach follows multiple design principles first function behaviour decomposition means that we should be able to distinguish between actual customer functions and elementary behaviours. For example, a level two lane keeping adaptive cruise control system could be split into two elementary behaviours, keeping distance from the leading vehicle and to centre within your own lane. If you're not familiar with these levels yet, level two means in a simplified way to combine two Ada systems with each other. As long as you don't encounter a cow which level is over 9000 you should be safe. The second principle is the distribution of interpretation. The situation analysis in this case is provided by the functions themselves as it is not available during architecture design and should not be required for the architecture design. Third is the centralization of decisions similar to a plugin container. These iterative behaviour arbitration module decides which of the evaluate the available behaviours is ultimately executed. In the example of the to the right you can see three behaviour functions, each of which has their own situation analysis and path planning block. The advantage of using multiple toughening blocks is that it is hard to make one path planner that works equally well in all situations. Easier to optimise a partner that works well in a subset of protected situations. This will become clearer when we look at path planning algorithms. Let's take a brief look at the SAE levels to apologise for the bad drug I made on the last slide. You can see here that SAE level zero provides warnings and monetary assistance level one provides steering

Unknown Speaker  15:23  
or

Unknown Speaker  15:24  
brake echo acceleration support. Level two provides steering and brake or acceleration support. Remember the two elementary behaviours from the last slide, there are three and four can drive the vehicle under limited conditions, and shall not operate unless all required conditions are met. However, they differ in the requirements that are made of the human where the human has to take over very swiftly in situations when there are three systems face problems. And never four and five shall not require the human to take over driving. Enough from level four is assumed to stop safely on the road if the human does not take over in level five is expected to be able to successfully complete a drive if a human could have done so. This audio explanation is a little bit simplified and shortened and the actual implications of the different level definitions are huge. So if you're interested, feel free to study these levels in more depth. I have personally worked on functions for all of these levels by now and each one offers their own fascinating challenges. Other sometimes related level systems exists as well but if you want to enter the world of autonomous driving, you should know what the SAE levels are. Otherwise during some discussions you will be wondering if you ended up in a live videotaping of CRS song level up. Now how does the situate of behaviour arbitration bloc decide which of the semi competing behaviours or function modules it should execute for parameters identified for each experience situation, ending up with a score that can be used for selection and where the weights of the different parameters can vary depending on the situation or the overall vehicle tuning. First, applicability means the ability of the function to operate. For example, a lane change assist that sees clear lane markings, roadside boundaries might set this to 100%. And if it only sees the roads or boundaries, it might set it to 50%. If it sees nothing, it could set it to 0%. Second, the desire of the function to operate in percent. For example, an adaptive cruise control system which tries to keep the distance to the next feeble constant to a certain selected value could if it was set to keeping 130 kilometres per hour speed on a free lane, set this to 100%. But if the lane head is blocked by an 80 kilometres by truck anyway, it could set its desire to execute to 50%. Third, the risk of the execution or the assessment of the risk involved in executing the behaviour, where for example, bad visibility conditions could increase the risk of executing the function. And finally, comfort implying how comfortable the execution of the function would be for the passenger. Imagine becoming seasick in your vehicle while desperately trying to finish your study homework during the autonomous drive to the university. After deciding which function or which combination of functions to execute, we arrive at the motion management block which connects with the actuator abstraction and computes the best control actions in order to drive along the path provided by this the weight of Abbey the AVR arbitration module, which could include steering front and back wheels torque vectoring, single wheel braking

Unknown Speaker  18:43  
or a combination of these. Finally, the HDMI management block connects the system with the human we have already seen with regard to the SAE level table, that the interaction between humans and the autonomous vehicle can be very important. For example, when you consider the time it takes for the human to successfully take over when the vehicle enters is now you that is beyond its current capabilities. The API Management also provides all the abstractions for the display options such as dashboards, head up, displays, augmented reality, etc. Especially important is also the safety management block which you can see also at the bottom right. And it's creating an autonomous vehicle that is socially acceptable and safe at the same time. Those are two of the key challenges for autonomous driving in general. The Safety Management module in robinus performs two important functions, which are to perform plausibility checks on transfer data and to collect all these plausibility checks. results in derive from them suitable strategies in terms of safety and error handling. And with that we have concluded our exploration of the Robonaut system and now let's take a look at it. A small, relevant related example of its application. We are now looking at a prototype that we developed for the release presentation of robinus. At the consumer electronics fancy es Las Vegas several years ago, we will take a brief look at the different robinus blocks that we implemented for the demonstration, which used motor vehicles to drive around test wrecks during the division. And we're looking at this to make the previous discussions a bit more concrete for you. And we're looking at model cars maybe to motivate you to also implement something like this yourself for get familiar with the the content. You see on the left slide the now hopefully familiar architecture diagram showing the use of cameras lighters, ultrasonic sensors, and a map for positioning or localization and written up fusion, followed by a Scituate of behaviour arbitration block with three exemplary behaviours lane following the emergency stop in the parking, converting these results into a longitudinal and lateral control and then forwarding it accordingly to the steering and motion motors. On the right hand side you can see a description of the small vehicles that we set up for this demo four vehicles were used on the test track simultaneously, each with the same hardware specification, a one to eight scale size vehicle with a brushless DC motors four wheel drive to its steering in order to be able to shrink test track and drive networkers headlights indicators reverse lights, Arduino boards to handle the sensor communication and pre processing a nice I seven main processing unit RGB D cameras motor encoders an inertial measurement unit to help estimate the orientation acceleration. into the bottom right you can see the orientation of the sensors, lots of sensors, looking to the front including the RGB D camera, and also some actual zoning sensors looking to the back and lighter looking to the right to scan parking spots.

Unknown Speaker  22:15  
Use an extended single track model to describe motion of our ego car. We will come back to this in the path planning section later. For now Just think of it as a physics model describing how your vehicle moves based on acceleration input. We use odometry encoders IMU to estimate the movement of the new car and this way, we placed markers around vehicle track, which were then detected with cameras to triangulate our own position. According to utilise map of the test track. The markers are here in grey, they are all placed where there are actual traffic sciences, you could imagine it as detecting the traffic signs on them on the map. You can see in yellow, the ego car, and its current camera field of your which I just marked for you. And you can see behind the ego car, the recently driven path in red as well as the originally intended path in yellow, which is measuring very well. And you can see in orange the lane markings of the track, you can see 1234 intersections on the track, you can see two parking spots. And you could imagine that there would be three other vehicles somewhere on the track where maybe one of them could be parking. And the other three vehicles could be driving around the track with no pre planned path. So at each intersection at every point in time, each vehicle was allowed to make its own selection of where it wants to go. We apply the simple grid freedom approach where each cell represented a one square centimetre area and could contain values from zero to 255. With 127 being an unknown state, zero being a free state and 255 being an occupied state, higher values being more certain occupation and lower level representing a more likely freezer. You can see in the map image at the bottom left how the car detects an obstacle in white here.

Unknown Speaker  24:26  
A very simple approach to convert this into a grid fusion would be to just add to your cell value whenever you measure a cell as occupied or to subtract from a cell value when you don't measure it as being occupied in the way of slowly forgetting what was there or to strongly subtract from the cell value when you effectively measure it as free with a sensor. We can return to this topic as well when we come to the fusion chapters we consider Three elementary behaviours, lane following emergency stops and parking. We estimated desire values for speed and steering, we computed the highest priority and executed the according behaviour. We obeyed speed restrictions, the light signals were said when making turns. Traffic routes were also a bait in the way of negotiating the right of way with other vehicles by checking the presence of vehicles within the intersections and following lanes was achieved by a controlling vehicle to steer towards the centre of its lane and to progress accordingly. You can see the mentioned lane centres in grey in the map image here you can also see the path marked by purple points, which was our path prediction for the ego vehicle, which was used in order to assess if we needed to brake or accelerate given the distance to the next obstacle. We compare the polygon representing the predicted path with the occupancy grid map and thereby estimated the distance to this next obstacle. We created functions to express the relationship between all those speed and execution desire versus the distance to the next obstacle on the predicted path, which you can see on the right. For example, a closer distance to the next obstacle would mean that we want to achieve a lower speed and that we have very strong desire to achieve this lower speed.

Unknown Speaker  28:27  
That's it for Robin nose exploration. Finally, short comment on the to highlight again the importance of good environmental protection, fusion and safety approaches. I want to show you two exemplary general scenarios. You remember the the ultrasonic sensors were all facing to the front of the car. However, in the image to the left you see a situation where the car on the back does not actually perceive the front car with ultrasonic sensors even though it is within the field of view, because some senses are too close while other sensors suffer from total reflections of their emitted signals at the smooth car surface. And coming back to the agendas of motion and decision making. You can see at the right hand image of a deadlock situation recreated during our tests, in which no car moves anymore due to the conservative margins introduced in order to avoid potential collisions. This demonstration we made in Las Vegas was a huge success. It had a vast amount of visitors and was featured in the 80s. That magazine also it's one of the key demonstrations at that year's automotive CES and how this let's exit the world of robinus and model cars again and come back to the robots and the big cars. By now you should have a basic understanding of typical modules used for autonomous driving, not so much the concrete implementation but the general interconnection and responsibility. As mentioned besides robinus other frameworks do exist such as autoware or Apollo. Consider the Robins architecture as one example. Next time we will shortly dive into vehicle hardware architectures, and robotics history to complete the base for understanding today's autonomous vehicle situation and to get back to algorithms for now, thank you for watching and until next time

Transcribed by https://otter.ai
