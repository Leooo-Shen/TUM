Unknown Speaker  0:00  
Or do you only use one current camera image and adding time dependence can be helpful, it can also cause more challenges during training of the network. And it can also cause more computational complexity. In any case, you should be familiar with the implications of being time dependent if your network or not. Auto encoders are very nice architecture that is used a lot in autonomous driving also present in every deep learning lecture. In principle, you have an input, you force the system to compress the input, and then decompress the input with the idea that the decompress input should then ideally, equal the input that you gave such that you reserve all the important information in your compression in between. and you can model this task to to many different applications in the autonomous vehicle, you can reduce snow in an image, or you can predict a situation you can draw completely New Avengers. The nice thing about these deep learning algorithms is really the versatility in the application. You should be familiar with deep convolution neural networks as those were the breakthrough networks in the work with images. And they have now since sometimes also been translated to graphs. So you can have large images or you can have large graph structures that you then run through deep convolutional neural network. In order to perform classifications or predictions. I just said you can use an autoencoder to draw, for example, entirely new scenes. But another network architecture that is well liked and you don't miss driving domain. And this is also like for this case, is the generative adversarial neural network. They are also the more complex side of being trained, but they can produce really nice results. And the cool thing really is the reference that you have in order to express the task for the new network by setting up one network that tries to fake outputs for another network that tries to discern if this is a true output, or false output. For this lecture, it's it's enough that you're that you're aware of the these generative adversarial networks. And they're very basic principles, but also an important architecture at the moment. We said that missing for the breakthroughs for the networks were the storage, the available data, the processing power, and so on. In research, also some mathematical tricks. If you make the network very long, at some point, you run into vanishing or exploding gradient problems. And deep residual networks are something that is used in order to prevent excess exactly these problems and enable it so that you can train even larger networks. And over time, the networks have grown larger and larger and larger and larger, with barely any stopping inside. And at the same time, more and more methods are coming up in order to, again compress in your networks. So you sacrifice some performance in order to be to have the network use less memory, less storage capacity that you need, if you want to save it, for example, on a mobile phone on a small road, and maybe to be able to apply these four replication strategies. Instead of going with well established architecture for a new network, you could also say, Well, if I learn the parameters of the network, why shouldn't I also just learn the whole structure of the network. And there is some interesting investigations by a team that Uber for example, well, you could say that neural architecture search is, is the future. However, at the moment, this is still very researching. And most people work in the autonomous driving domain, tend to prefer to go towards some well established architectures where you know that the principal performance that you could be expecting of the of the neural network, and then you adapt that to the case. Whereas for the neural architecture search, you might need a lot of computation, train time and so on, even to actually get to the point where you have like a promising architecture identified but still requiring future research to take direction also. Let's take a look at just one example from 2017. So this is a small work I did with Mercedes Benz in the USA. And

Unknown Speaker  4:52  
what we wanted to do here in this case, and it's nice, it's published because most of this works typically on a nondisclosure agreement that the person You can always talk about. And the idea was that we have a situation here real world. And in this weird situation, there could be multiple cars, they could be road topologies, they could be false positive obstacles, extra road, whatever. And you have a representation of this data because it has already run through a detection module. So now you have an object list and this object is contains three vehicles, and this is now easier to see drawn yet. And for each vehicle, it has a velocity information and type information. So is it attractive? Or is it a small car and so on? And then the question was,

Unknown Speaker  5:41  
how can we

Unknown Speaker  5:44  
classify the scenario that we are seeing here? Is this an overtaking scenario? Where you're overtaking somebody? Or is this scenario following somebody? Or is this a scenario where we'll have to break, you can identify different types of scenarios. And at that point in time you wanted to try it with convolution neural network. The nice thing about convolution neural networks is that you can have any amount of cars represented in this one grid cell that represents the the x and the y of the of the scenario. And you could, for example, run multiple layers through your convolution neural network. Same as you can have one layer for each colour, you can also have one layer for each type of information, if you so desire. So one thing that we've named the velocity grids at that point in time had three layers were one of them was the free input layers, one of them would consent in the occupancy. So this is an occupancy group map, where you have different cells occupied the vehicles or by objects, it contained the x velocity for each point and the y velocity for each point. Another experiment we did was what we called a stacked velocity grid. So we would have the same representation, but we would have it for four different time steps. So T one, T two, as many as you want. This is a cheap way to introduce the time dependency into the network, instead of using a recurrent neural network. At that point in time, I actually suggested using the current neural network that the partner of mine in the United States went to the Stanford University, and they had a PhD soldier law don't use the recurrent neural network is so complicated to train. And so ultimately, the decision was to go for the the coalitional network approach here. But the training commentaries was always accelerated, you just need to invest the time that you need to train it. And nowadays, it's also very known how you need to train the different kinds of managers. And finally, we had a history grid here, where we decided to into one layer, merge tag information also. So for example, you could have the previous sensor also occupied and the longer a goal was occupied, the smaller its occupation value would be. And this you could then in one layer Express time, we're just curious what kind of results we get. Back in 2017, here, we chose a supervised learning approach. You have supervised, unsupervised, semi supervised, and reinforcement learning approaches, all have their own upsides and downsides. to stress the right tool for the job and then apply the disciplinaries. For the individual type. Here we're looking at a supervised learning sample, which means that we need label data. So we implemented a small labelling tool for ourselves, these labelling tools can save you a lot of work if you have a good one. And we defined a number of scenarios, some very easy scenarios, some more complicated scenarios. So the easiest scenario is more to see does network learn something at all the more complicated scenario in order to estimate Where are the limits of the scenario, and here are the black ones and the white ones in the markings you can see which is now use active. They can also be multiple scenarios active for one data point and you see here the real data of the objects moving and and you can also say well, but isn't this very messy, like velocity cell over the place? If you look at this as a human need to say what scenario This is, it can be complex and that's often the case for the inside it's complicated complex to to estimate this situation from this. Another thing that you should be able to read if you go into this domain and you work in artificial intelligence components are network architecture structures. So you have these two different layers that you go through all with standard names, those of you who are very familiar with neural networks, you will be able to read to already interpret each one of these. And ultimately, you have up here, the different input types that we just explained in the last slide. And from this, you get some nice predictions, we say, Okay, what kind of scenario is this?

Unknown Speaker  10:23  
I think we already introduced before the receiver operating characteristics curve, which has the false positives on the one axis and the true positives on the other axis. And you should already know that it's nice if you can get here to the top left or as close as possible. And you see here that we have a list of the different scenarios where this vendor frame just means, yes, there is the data and the frame, which is the case for most of the frames. And you see, this is almost perfectly detected. And then the most difficult scenarios here that we say that other vehicles are now overtaking us, that one is more difficult to detect compared to the weather frame data. And However, on all the result was quite nice, and then later, we added more scenarios. And

Unknown Speaker  11:17  
what you can take with you here, again, is that it's nice to have metric, in this case here, the RC curve, which you can use in order to evaluate if your algorithm that you implemented matches your your goal. And then it's nice to have a mixture of simple and hard scenarios. This is not only the case of the deep learning approaches, but whenever you develop autonomous driving functions, you typically get a selection of scenarios that are very difficult in a sexual scenarios that are very easy. And somewhere in the mid range, where even if you're working towards the medium scenarios, you always have the more difficult scenarios in mind. So that you can maturity or your architecture development and so on also against the hardest scenarios. And you know, when you have to take this in the future, we're seeing more and more officers also in the cars. If you think about all these artificial intelligence approaches, there's a good amount of officers also in this domain. And now, the question is how can you use this in the car and this is a photograph that we just took ourselves and you can see here that as you turned on the car, you were bombarded with a very long licence agreement about officers. And so, this is this is becoming more and more important using open source gives you so much velocity that even when you develop autonomous cars, now you have open source components quite often somewhere in the system Alright, that was it with the neural network topic. So again summarising please be aware of the dominant architectures, the ups and downs of the dominant architectures, and the ways that you can the different ways that you can train these systems. And it is a bit to how they how they can be used in the autonomous driving domain. Data fusion is the next topic and also the last big topic. So we will first talk about what data fusion does, then we will talk about approaches to data fusion, and then we will conclude with the some with the most important tracking algorithms to give you a base they also. So first let's talk about what data fusion is. You can find more about the CMD data fusion Handbook, which is a very light tempo for data fusion, but does not go very deep into the algorithms. It says a data fusion is the process of combining data to estimate entity states where an entity can be any aspect of reality at any degree of abstraction. Or basically you have some measurements from a, from a car ahead of you. And you want to know Where's the car? And how fast is it just as an example. We are talking about a multi level process dealing with the association correlation combination of data and information from a single and multiple sources to achieve a refined position, identify estimates and complete and timely assessments of situations, threats and their significance. And we'll get back to the training model in a moment. Here's another definition from hardiness. Data fusion techniques combine data from multiple sensors, and related information from associated databases to achieve improved accuracy, and more specific inferences that could be achieved by the use of a single sensor alone. The goal is to transition from Contributing data to reliable information to obtain a lower detection error probability and a higher reliability by using data from multiple distributed sources. So over here on the right, you can already have a teaser where you say you have multiple measurements with different covariances. And then you combine them together to receive an updated covariance of your, your object that you're trying to estimate. It's the state of there's something which is called the data fusion secrets. And this is not so bad to have heard about. Because these points are relevant in almost every implementation of data fusion that you run into. There is no substitute for good sensor. So the first question you run into is if you have better data fusion, can you maybe spend less money on the sensor in order to create a cheaper product create a cheaper car. And it's still nice to have really good sensors, if you could have perfect sensors, you wouldn't even need to have data fusion maybe, because the sensor already gives you everything you want. We use data fusion because there are weaknesses in the sensors. If you make the sensors, we can we can we can you just make it more difficult for the data fusion to do a straw. So a good sensor is very valuable.

Unknown Speaker  16:17  
The next sentence also goes if you use a really bad sensor, or you do some really bad pre processing, the downstream processing cannot absolve the sins of the upstream processing. So the information that you have lost early in the chain is very difficult to reconstruct later on the chain. So if you want to improve the the perception of the tracking, start from the beginning and make sure that every step is up to quality. Fusion has the goal of creating an output for multiple sensors that is better than the individual sensor alone. However, only because you fuse or use fusion algorithms in order to combine multiple sensors, that doesn't mean that you mess the whole thing up or that good sensors pulled down, but the best sensors. And so the result can actually be worse than just the best sensor result that you have on your own. There are no magic algorithms. This is the case for everything we discussed in the whole lecture series. You have these different path learning algorithms, you have these different neural network architectures, you have these different sensor policies, different fusion and traffic approaches. And cubically it's not the case that one algorithm clearly dominates the other algorithm. But instead, a lot of depends, depends on how well do you implement the algorithm? How well do you parameterize it how do you combine different algorithms. However, for certain situations, certain algorithms can play to their strongest advantages. So for the algorithms that you choose to use, you should be very well aware of how they are what their strengths and weaknesses are. They will never be enough training data. Which is, of course, true, you can route and train data such that it becomes long awaited isn't really for you. But there will always be situations where they will be nice and recording of this and then you don't have it. So it's good to record something at the same time from the GDPR you're supposed to recall this little as is necessary. And then people will ask you how much data is necessary in order to train your algorithm. And then if you say there's never enough data, the gdpi force will not be having to sample for you to give them multiple medical and then come up with some incremental approach where you define a certain amount of data that you need. And you train with that. And you see if you run into a situation of you benefit from our data, it is difficult to quantify the value of data fusion metrics that clearly expressed the value of your fusion algorithms are hard to define, and how to evaluate and finally, fusion is not a static process, which could mean that you're your situations are different and SEO react to different situations, you can then also use different algorithms or adapt them and you can go through this through scenarios step by step to improve your convergence or you can also prove your overclocking processes. This is a very generic statement to do, they can have many meanings, but all in all, here this this list of different secrets is helpful for your later fusion experiments. One thing people often say in your tournament driving domain that you want to go from data to information. So if you do raw measurement, what is that this is data. And data is not so useful in the end because Do what you really want is information based on which you can make your own decisions. So here's a diagram that can help you explain this a little bit. Examples for data are that we could have raw signals, or numbers or letters, or symbols or strings or colours. And information is something that is supposed to be of the highest semantic value. So you can ask questions like who, what, when, where about about the information. And in the past, we didn't have enough data and we didn't have enough information. Nowadays, many car companies, for example, have dozens of petabytes of data. So they have lots of big data and we but what they are missing is information. How can you extract the information from this big data, or maybe later, we don't have these directions of going from information to knowledge of wisdom, the at the moment, most of the discussions about going from data to information.

Unknown Speaker  20:58  
There are some related terms, for example, information fusion, or data fusion, which are often used as synonyms of each other. And sometimes data fusion refers more to the raw data information fusion sometimes refers more to the already processed data, information fusion might imply a higher semantic level and data fusion. And there are other ways to talk about data fusion decision fusion data combination data aggregation and multi sensor data fusion sensor fusion. Data fusion techniques are often employed in multi sensor environments, with the aim of fusing and aggregating data from different sensors. But they can also be applied in very different domains, for example, that you have multiple texts, and do you want to fuse information from multiple text sources. So the techniques translate relatively well. Now we're going to talk about some various approaches for data fusion. Let's take a look at this one, first here from the Jdl model, which has the advantage of being relatively well known and accepted. But many options are also possible to go for. So here on the left zero, we would have sub object data Association estimation, which is very low data, like pixels and signal levels and so on, then we would have the level one, where we are now talking about objects that we refined. So we're now talking about objects that we are tracking, we're trying to estimate the state of the objects. So we could have the kinematics of a car that we're trying to estimate, we could have a classification for our target, and so on. And then on level two, instead of talking about the individual object could talk about the whole situation, trying to cluster multiple objects together, understand how they belong to each other, and how they could communicate. On the next level, we could have the significance estimation or more military saying that the threat refinement, so we're trying to understand intends to trying to predict the situation, to understand the consequences that may happen, think about where the vulnerabilities are. And then on the fourth level, we can even talk about the process. So

Unknown Speaker  23:22  
we could

Unknown Speaker  23:25  
update our fusion processes

Unknown Speaker  23:28  
have

Unknown Speaker  23:30  
the even more adaptive approaches to the two scenarios that we have selling. The JV animals have, from an architectural perspective represented sets such as this. So you have two different levels 1234, up here, and then you say, maybe this is connected to a database. And maybe we have an agent, I use a machine interface to communicate with the human, and maybe have some data sources and so on. Here's again, a nice example from publication, where on level zero, you have these signals from the cameras themselves. Then on level one, you have the individual objects with a path that you're tracking in the classification. And then on level two, you're now trying to understand the situation like if they already passed zone one or zone two, where are they under parking spot? on level three, you maybe do the threat refinement, where you say, Okay, this is a problem that they have pastors and then on level four, you can have this high level thinking is maybe we need a new algorithm, or we need to exchange the sensor or something like this, in this particular publication from which we got this example. It focuses on using contextual information to improve all of these various steps which you can of course do. And when you talk about context, you can see how that influences the outcome of Your state estimates. And this can, for example, be the case if you say, well, it's rainy and it's dark, then you know that your camera seeking quality may be negatively affected. Or you could speculate here on the threat estimate perspective that if the day is clear, and it's broad daylight, that it's not likely that cars actually stolen. On day one, with the rainy weather again, you can see that maybe the cars will drive more slowly. On level two, you could talk about the general situation, say, okay, maybe the traffic is more intense. Sometimes people like to categorise these fusion levels into low level fusion and Island fusion. So you could say, talking about the signal or the individual objects or slowly the fusion, and then talking about the complete situation, asking how to whatever your abilities, the rest and predictions of the into the question of how do we even perform our data fusion process is the high level fusion. Just briefly point out that you could also order context information according to its obstruction rate going from low level to high level. So for example, saying a map of the area is on a lower abstraction level than general access statistics of your parking spot or the relationships between unions again, or on an even broader level, the available hospital resources in the country of for example, in the entire diplomatic situation of a country. Let's take a brief look at possible data fusion architectures. So there is the Bowman architecture from 1980, who studied the Jdl model that we just discussed, and said that, while the structure of a dative L is useful, and he wanted something more concrete in order to ask how can this actually be implemented. So here you have a train where you're first detecting something discrete. For example, nowadays, the new network detecting a car in an image of road, or this could be a laser scanner, measuring a distance an object, then you are predicting where this object might go, you're associating the measurements with this prediction is let you know which of the measurements belongs to which of your traits that you're tracking. You're saying, Okay, this is probably a car and that is moving in a certain ways. Now you have instantiated, a new object to be tracked. And now you can have a new measurement with which you then update this. This hypothesis that you have instantiated so that Do you think there is one car and it is moved like this, they need to be the measurement component, because you will probably not just have a one object that you're tracking, but maybe in your image of the road, they actually bought the cars, or maybe you have multiple hypothesis. So you say, okay, perhaps this is a car, but it could also be a pedestrian. And you can have them these water pipe offices until you have falsified all the ones that are not correct. And you remain with just one hypothesis, and then you can run this in a loop step by step.

Unknown Speaker  28:29  
Now, you can, on the last slide, say we talked about how to incrementally run all the different steps of a tracking approach. This slide focuses more on the question, if you have multiple sensors, how can you actually combine them and Lou and Kyle suggested the structure here, where you combine the information from multiple sources. So you have here the sensor, one sensor two sets of reasons for at the bottom, and they are combined here in three distinct fusion steps. During why there's another well known approach for robotics with regards to fusion also, where the data that you are fusing or the features diffusing are again sorted according to abstractions. So on the one hand, you have the raw data input, which you can then convert into features, you have the feature input which you can then bring up to the level of decision outputs. And you have to decisions with input which you can then find this order to make decisions of your reward. Another example from power from 1988. You have four sensors or n sensors. When where for each sensor, you first extract all the features. And then once you have all the features, you basically don't care about individual sensors anymore, but you say okay, which of the sensors belong to Which objective we're tracking. And step by step from this also the complete chain until you have combined all the sensors into one representation of your environment. Here's a road architecture from 1998 from the the laboratory analysis architectural systems. And you see, again the day liked cluster the sensors here into different groups the day abstracting similar as an overdose away from all these sensors and actuators, and you then go up to the function level where you have your perception in the different modules and then the whole decision making and motion and so on based on the sense of motion. Alright, enough with these base models, let's talk about some data fusion methods classification. So, we want to talk about the drawn right classification where fusion can be complimentary it can be redundant or cooperative. We will get back to this in a moment. And we also classified already fusion efforts based on the extraction level of the employee data when we talked about the raw measurements, the signals, the characteristics or the decisions, we classify as fusion methods by the different data fusion levels as defined in the Jdl model. We are going to talk about centralised decentralised and distributed architectures in a moment. And we talked about the sorting the fusion efforts by the inputs and outputs, for example, talking about raw data, the features, decisions, and so on. Okay, let's take a look at complementarity, redundant and cooperative. Complimentary means that information is provided by the input sources that represents different parts of the scene. So one sensor might see one thing the other sensor might see another part of the scene. And we use this information from to obtain more complete global information. So we could have a visual sensor network where the information on the same target provided the two cameras with different fields of view is considered complimentary. So maybe one cameras is one side of the car, and the other cameras is the back of the car that we're trying to track. redound now means that we have two or more input sources that provide information about the same target. And they can be fused together in order to become more confident. So for example, if you have two velocity sensors that are observing a target, you can then combine the estimates of both low velocity sensors in order to be more certain about the actual velocity of the target. Or a cooperative approach where the provided information is combined into new information that is typically more complex than the original information. So for example, combining audio and video data, as in listening to humans, and at the same time observing the human camera. In order to understand what's actually going on with you, he has the same thing represented as graphic. So you have the information A, B and C that make up a scene. And then the complimentary fusion, you have got the sense of one and two, where one sees information a two sees information will be you put the two together. And now you know a MP about seeing

Unknown Speaker  33:27  
the redundant fusion you have two sensors that observe information v. So you're more certain about B that you're observing the cooperator fusion approach, you have the information see where it's seen in two different ways, but two different sensors. And you combine them together, for example, the video and the audio of the human in order to get the complete picture of humans to fusion. Then we said we can classify fusion according to the data type standard. So we have here the data in data out, or we can bring data in and get a feature out or we can bring the feature and get feature out. We can bring a feature and get a decision out. Or we can bring a decision in get a decision out. Feel free to take out longer descriptions here on the slides. If you want to study this not at all again, we can classify the fusion according to which level of abstraction our datatype says so if I'm working on a similar level, where you only have like a raw signal, for example, a voltage or read up on a pixel level where we're talking about having a whole image that we are trying to work with, are we working on separate touristiques of objects, like the shapes of objects that we're trying to fuse in order to estimate the true shape? Or are we talking on on a moral symbolic level where we have information represented on an abstract level are very similar in separation to low level fusion medium level fusion Island fusion and what is the level of fusion, low level more focus on the raw data, the medium level more focus on the features like shape, textures or position, the high level when we're talking about decision fusion or symbolic approaches, or the combination of multiple levels where we bring together all of the above. All right, now let's take a look at these centralised decentralised distributed and deirik architectures for actually realising fusion systems. In the centralised architecture, we have a fusion node that is in the central processor, and this one receives the information from all of the input sources. And then all of the fusion processes are executed in the central processor using the raw measurements that are provided by the sources. So the sources only obtained the raw measurements and then they transmit them to the central processor. If we would have perfect data alignment, perfect data Association, everything is performed correctly and require time to transfer the data is not significant, then these decentralised architecture is theoretically optimal. However, there are some problems with real life systems. So we're taking all the raw data from all the different sensors moving it into one central system. This requires a large amount of bandwidth that is required to send the raw data for the network. This is a significant disadvantage for the simplest approach, as opposed to first pre processing it, reducing it in size and then providing that to the fusion unit. And if you look for some a few visual systems, with lots of cameras, the amounts of data from the different high resolution videos can be excessive, the time delays from transferring the information can also be significant with regards to results. Here's what it looks like you have each of the different sensors, they only do some basic pre processing at most and then everything else is done here in the centralised node which then publishes all the SMS phone fusion process. Then on the other end, we can consider a decentralised architecture. Now we have a network of nodes where each node has its own processing capabilities. It is not just a single point of data fusion. It it's not in typically fuse its local information with the information it receives from its peers. And each node can also act autonomously. Yeah, one disadvantage we have is the communication costs has a lot of information as B to be exchanged across between different nodes which could then also lead to scalability problems. So here you have an example again with three sensors. And the information from each sensor is provided to each of the different fusion nodes and you have three decentralised fusion nodes, which each perform all of the fusion steps redundantly and provide the state estimates to the outside world.

Unknown Speaker  38:26  
Then we have the distributed architecture with the measurements from each central process independently before the information is sent to fusion load diffusion of denic counts for the information it has received from the evernotes and this means that we can already perform some data associations and state estimation and so on before providing this information to the centralised fusion out there for each node provides an estimate of the object state based on the local views that they have. And this information is in the input for the fusion process in the centralised node or the back end attempt creates a fuse global view.

Unknown Speaker  39:09  
So this type of architecture provides us with a good number of opportunities for during different scenarios. So here you see again, the three different sensors about where for each of them, a local data fusion is performed with all the required steps. And then the output of this is provided to the central node that reduces the bandwidth problem because you've already gone from data to information on each individual node. hierarchical approaches exist also where you have combinations of the decentralised and the distributed approaches. Controlling can be said that again similar as to the algorithms there is no single best one beats at all medical data fusion architecture. different structures have different advantages, understand what they require. means for timing requirements you have, and then choose one accordingly. Similar as we did for the neural networks, it's let's take one look at an example that we implemented in the past. This is the infrastructure system that is indeed, the highway a nine. And you see that we have cameras looking down the road. And one camera that is oriented at the closer road distance one that is oriented at the far away road distance. And we have a radar, the radar detections are represented by the blue cubes here, the red detections are from the cameras. And what we do is we fuse every, all the measurements from each individual cell sensor separately, and then we again, we fuse all of the outputs together in order to reach one unified and environmental model. We then also combined this with a vehicle on the road. So here you can see in turqoise the detections of the LIDAR of this autonomous vehicle that we have in green here to the right that we set up with electrodes on the highway. And again, you have the blue red indentations, the red camera detections. And over here we left in also some false positive reflections over there, the world justice, warrior parking arrows coming from like most likely, from the reflections of the radio signals from the truck to the wall II of the of the highway, which in this case could easily be cut by assuming the boundary of the road and then ignoring all the points next to the road. using multiple points of view, as we do here on the highway, I have seen dealing with occlusions. See for some years why truck, which over there is difficult to see. But it's very easy to see in the other camera, which now we want to take a look at the different blocks of the fusion process. So first, we have the data alignment, the data that we received May, for example be misaligned in terms of the time domain, so the signals may not become time synchronously. Or the calibration of the sensors may be offset that position don't match the different measurements. Next, we have the question that we have different measurements and different objects that we're trying to track and go on to understand which measurement actually belongs to which object. Then finally, we have the after we have associated the measurements to the objects, we want to be able to use those measurements to predict the expositions and to always estimate the state of the objects that we're tracking precisely. So briefly getting into the data alignment, let's say we have temporal differences between our different signals. One thing we could do is use a shared clock. So everybody has the same information, the same time stamps, we could also synchronise via communication protocol, some return Ethernet networking, for example. And if we don't do either, we could also say okay, maybe we can automatically detect certain patterns in both signals that have the temporal difference, and then align them accordingly to overlap the patterns that we detected them both. And you could also consider this from from a logical perspective, and you could say that one event must precede another event or the two events should happen simultaneously audit one events ends another or starts another or the two events could overlap. So this is how you could express different temporary different scenarios.

Transcribed by https://otter.ai
