Unknown Speaker  0:07  
We have now seen that there are certain scenarios in which sensors perform better and which sensors perform worse. And we will spend a little bit more time to get more potential pitfalls across to you. All the things we're going to look at now are just examples, take them with a grain of salt. Don't focus on this specific sensor. But just be aware that there are many things you can track if you integrate a new sensor into your vehicle. So here for example, you have laser sensor, a ladder, which is placed at a certain stationary distance from the target, and the distance to the target remains constant. However, if you look at change over time, here, you see that the distance is decreasing while it should remain stationary. So this could be the case due to heat dissipation of the sensor, many calls possible. For now, you're just seeing that certain things that you would remain to be constant, I did not actually constant and practice. Same again, you would assume that if you measure an obstacle, and the only thing you change is the way that you have your own sensor on the table at a distance from the obstacle. So for example, you are rotating the sense of upside down, or you're putting it on its left or right side, you would assume that this has no impact on the distance or the distance. And you can see here again, how the distance is different for this lattice. So depending on how you tilt the centre, you would assume to receive the same distance as a measurement independent on if you measure it, the obstacle during the day or the night. Again, we hit this test setup that you saw on the first slide consistent. And you see here that depending on the day that you have, or tonight, and the relevant angles that you use to orient this plate that you saw on the first image, you get different distances measures for the centre. Also for the surface brightness or colour of the target, you would assume that this does not impact the distance. However, you can again see here the four different materials that were tested, the lighter gave out different distances, you can see the same effect for different target materials. If you want to, you can also read the original paper on the distinction at the bottom left, and study this in more depth. It isn't relevant for us here in the lecture if the distance changes are or they aren't absolute values. The purpose here is for you to see that many things where you would say it's intuitive and obvious how this work, then can then be different when you test this as on practice. So here again, you change the incident angle, and you get changes in the distance as well that are larger. issues that arise from these things for autonomous vehicles gain a lot of attention, as you can see, by the example of one of the early videos of Tesla autopilot here, which has the title of Tesla autopilot tried to kill me and also resulted in a good amount of clicks. Tesla is only an example here. You see other examples from weyburn co as well. This case was it has a driving driving on a strongly curving roads, countryside road with the vehicle ultimately maybe pulling into the direction of the oncoming traffic and the driver disabling it. There were many other examples that went away to YouTube. cases where there was like rain, for example, and the vehicle failed to stay on the road and touch the the area next to the road with the wheels fortunately didn't come completely off the road. It was very typical for these cars to exit the highway accidentally. So even if you follow the manual that says you're supposed to use the highway, if you just turn it on without paying attention to drive off the highway if it doesn't understand that there's an exit. Some things like leaves can fall on curve stones, obscuring them so you lose your clear distinction of where your road ends, the road can change its shape and it can become narrower or wider, which also caused the car to lose track of the lanes. These vehicles has had and still have difficulties with very small obstacles like traffic cones and so on. or specific stops that are no cars like a barrier being put on the road and

Unknown Speaker  5:04  
strong curves were often still challenged. And yet so many sensor cases existed you have to be aware of if you design the operational design domain of such a function. Changing scenarios were often introduced as functional insufficiencies of the sensors. This is especially complicated if you have multiple cases combined. Here, for example, you have the fog coming out of the ground, which could cause errors for the cameras, the laser scanners and so on. And simultaneously, you have the metal cover the manhole, out of which the fog comes out, which could be detected as an obstacle by for example, the radar. This images to suppose indicate that, for example, for cameras, you when you track features of your environment, so you see how things are moving with the camera. If you have big areas of the image that I've almost no features where everything is just white, it can be changed, it may work in this situation here. But it can be challenged if you're trying to understand how the picture is moving in terms of optical flow while you're moving. If the other features are the distinctions in your image. If you search for objects that are difficult to detect Another thing is if you have these petals on the road, and you fill them completely with water. This also leads to interesting sensor physics combinations, so you fill them completely with ice of thin ice layer. On the one hand, these things are difficult to detect and to separate from false positives. On the other hand, they are now also business models where cars specifically made for detecting these things driving on the road with powerful scanners. And they then highlight these areas to the traffic institutions such that they can be fixed. We already had the smoke from the manual cover. Of course, depending on where you drive, which country and so on, which people you're driving with, we can also have smoke from other vehicles that can obscure sensors. And similar to smoke, you can also have water. So you can almost spilled the water wall that you then trying to pierce with the sensor signals in order to detect the car behind it. And they also works studying how much water on the road which philosophies and so on cause which kind of impact on the sensors. But in general, these effects are not so well understood yet in their practical implementation implications for the actual performance of the market sensors for autonomous vehicles. But you need to keep those things into concern if you drive in such a manner. This image is most likely fake a fake image from properly Australia. It's just supposed to illustrate that you can imagine the wireless scenarios and if you drive enough kilometres, you run into them eventually. If one of your sensor modalities is negatively affected by the environmental situation, like here, for example, the traffic light being obscured by the sun blinding, it can be helpful to have another modality to to detect the objects that you want to detect that is less affected by the situation. Some objects that are meaningful to humans and that only have one word to describe them, like the speed bumps here can appear in very, very different shapes and sizes. So training detectors to detect all of them can be a challenge. At the same time, they also age and the appearance may disagree or degrade over time. They can also have a very small hit over the road of only a few centimetres, which can be a challenge for sensors detected long distance for can also hide traffic signs, so that they only become visible when you're closer to them. And you can see them when you fell away. To study these effects, you can use simulation or you can try to set up experiments or you can try to get into the real traffic scenarios where this happens. Here you have an example of an artificial fork that has been created to test the car and on the one hand, it's obvious to you probably that this is not a very, not a very realistic form. It's not easy to create a realistic focus on an experiment on the road. They haven't it's interesting to also hear see the impact on the sensors. So you see here that the

Unknown Speaker  9:43  
I drew in an orange line here to point out the differences to you and you see that you have these white stripes that's the arrangement letters getting detections compared to the other image where the car is still in the fog where you have nothing at all. It's easier to see on the next slide. As the car enters the fog, you can see In the middle that it almost sees nothing towards the front. And once it's out of the fog, we can see everything again. However, this is a very special kind of forecast, it's a super dense fog to the phone is active for a few metres. As this is the best way that for this competition, they could set up the experiment. Briefly touching on snow on the ground, you can see here a bicycle driver. What you're supposed to take away from this is that not only is the road obscured, not only do the sensors get dirty, and the lanes are hidden, the traffic lights are barely visible. But they can also be consequences as for example, the emotion models that you use to predict and model the motion of the traffic participants becoming an adequate if for example, the bicycle driver now falls over and becomes a small obstacle on the road that needs to be avoided for safety purposes to prevent any damage. Then, of course, there's art art is beautiful. And this traffic light three here is a beautiful well known tree. However, if you ask yourself, can this cause false positives for current camera detection systems, then the answer is yes. And then you could consider if you have a high resolution app that you just mark the position of such a camera tree once you learn upon it, or once you return upon it. But also, this specific traffic light tree, as you can see from the slide was then moved to a different position. So into even the map would have to be updated. So what you do is you run a lot of tests, and from this slide, you're supposed to take away that even if you test sometimes these events are to catch the traffic lottery, maybe in a very special location. And here you have now an architecture that causes very strong reflections of the Santos drivers to downside is, as you can see in the next slide, this doesn't happen very often. Instead, this only happens two times per year for 20 minutes per day, approximately at nine o'clock. And the blinding is strong enough to cause temporary blank spots for the driver version. And you would have to test here during these two days if you want to find this specific instance, coincidentally, by a random test. So what we like to do instead is think about this phenomenologically and blandings by the sun is something that you can expect. And that you can then try to catch if you're putting attention to it. overhanging obstacles can be a problem. If you imagine that you would, for example, have a laser scanner that has a horizontal field of view, that might not catch the overhang obstacle. In this case, that's a Tesla and you have got a radar, and you've got a camera. In this case, the tester was blaming the driver for being responsible for creating this mistake, for example, not having the preventive systems active you can check out his turned out later. But from the static awaited obeying obstacles, our challenge for autonomous cars and robots. When you're programming safety critical software, you always need to be careful that you don't introduce bugs. So here you have a bug, where the newspaper article for this was called Tesla autopilot knocked out by a bug or an off, which is peculiar until you've got the radar that was malfunctioning due to the back on top of it. More bugs can cause issues. So you know that this traffic light here the orange, you can see that above the one, the red one is pretty much hidden. So what is the impact of this on your detection igraph. It's very important that you learn debugging with regards to autonomous driving, so that you can have the traffic lines visible again. In any case, if you want to have a serious comment, it is quite normal for software not to work, especially if you use open source software. And then there are many people who just say, oh, it doesn't work. I tried random stuff, I don't know what's going on. And then they are much less people who know how to debug software appropriately into can get to the root cause of the problem. So becoming a good debugger is a direct value increase for you with regards to occupation in this domain.

Unknown Speaker  14:32  
Because every individual system is typically flawed. These autonomous driving systems tend to use multiple systems that are then combined. And in this case, you can see a truck being followed by a Tesla at night. with mild rain or snow weather you're also losing the lane markings then and if you lose the lane markings that you're following, you can instead consider falling back towards Using the truck, and just saying, Okay, well, if I follow this vehicle, that may already be sufficient. However, for example, if there's a curve, and the truck follows this curve, and you fall behind, and you think out the track is just going to, to the side, you may also follow to the side, you may actually exit the lane. Instead of following as you as you would have been supposed to, there are many detailed discussions about how you can implement these driver assistance systems. In the key point is takeaway that you combine multiple systems such that they can then support each hour, but you always need to be very have edge cases that can cause issues with you that you weren't expecting. The answer for this, then is that you need to test everything. In the beginning, when you make a proof of concept or prototype. It's already good to have unit tests, integration tests, system tests, smoke tests, and you should also know what these different tests mean, which you can probably look up for our lectures. But in principle, if you want to programme safety critical software, such as autonomous driving software, you need to test your solutions. If you already have a system in the field, and you want to make updates to it, you need to test the solutions before you patch them in. The trend definitely goes in the direction of saying that every component, the vehicle should be updatable. That is the expected future. And that means that updates can also cause damage to our components. And if you have a system that you want to improve, you need to consider that. So just again, for the people who are curious into looping unit tests, which are these testing the smallest unit, you can check in more depth than integration tests, bringing multiple things together to test if they also work, not only on their own, but in combination with other parts. And system tests means that you bring the whole system and you test the car, for example, a smoke test roughly means that you take the whole system and you expose it to a typical customer use case and see if that use case works out well. The more in depth explanations that you can check out also the this slide here talks about the messiness that is driving on the road. And the driver was trying to feel how alike the passengers trying to feel the quality of the motion of the vehicle. And you can see here that he says that the visitors would meander, which means it's it, it goes from side to side of the lane, from one side of the lane to the other like a pinball. And that is does not seem confidence building. So the performance of how well you perceive your environment and how you react to it directly impacts the the customer research. You can find my error sources if you're interested in the various autonomous driving reports that have been published over the years. So you can see here, for example, that they were disengagements for Google because of weather conditions, because of other cars behaving recklessly because of problems of the Google hardware, because of the old car performance of vehicles that were undesired. The perception being inconsistent. The average traffic not being predicted correctly. The software having unexpected problems, construction zones being encountered that are more complicated to handle emergency calls coming up that you need to make move on. Alright, enough talk about typical perception problems and so on. What do you want to do now is get you to the point where you say, Okay, you've got a basic grasp also of how to go about using

Unknown Speaker  19:21  
more sophisticated models for sensors, we will look at this example for the LIDAR and go through it swiftly. So this is something that you can implement yourself. It's not too complicated. And it's it's nice to know how to do really good LIDAR model. So, again, we come back to this question that we say, if we know how good reflectivity of a target is, then what is the distance at which we can successfully detected and the better or liners, the service or detection range and the better our target is the further or detect range and the more optimal The environment conditions are the betters or detection range. So you want to have nice mild, Sunny daylight weather, or with no rain, you want to have strongly reflecting targets that gives you a nice diffuse affection back to you. And you want to use a very good sensor just to give you a feeling they have audiences discussion, let's take a more precise look. To understand how far in obstacles or target is away from us, we can just emit a signal that hits the target, then wait until it comes back to us then say that the signal travelled with the speed of light, multiply this with the time it took to get back to us and divide this by two and we will know the range to the target. So the distance to it. With liners, this is very nice, we can have like a sharp laser beam. And that ideally only hits our target point to measure. And there's a there's a nice paper about this model, extended simulation based material source. Versus here they have two papers, influences of weather phenomena on automotive laser radar systems, and it's from the company's piece and from BMW to some really nice experiments also got to meet them in person in the past and discuss how we could do improve laser certification on highways and some very interesting history. But this look at the implementation. There's something which we call the LIDAR range equation, this is supposed to tell us that so what what power would we expect to get for target at a certain range are and this depends on some system constants that describe or lighter that we can then measure through experiments. For example, it depends on how much power we have put into the signal and the shape of the signal of how we have trance transformed it. And then it depends on how the signal travels to the target how much it is disturbed by the environment. And how much is has been reflected back to us. So we will look at these parts also in more depth. So first, we assume that we just give out a simple Dirac impulse. It's easy to model here. And we don't get this perfect signal in reality and purpose, it's good enough for staff to model and we have some limitations for the models, for example, that the minimum distance that we have to take into concern is relevant. So for this model, for example, you would like to target to be further away than three metres to six metres. So for the signal that we are emitting, it's relevant, how much power do we put into it, how long do we emit that power, then we get the total energy that we have emitted towards the target. Now the spatial impulse response of our optical channel that connects our sensor with our target that we are trying to measure. This can then be modelled as a combination of the impact that the target has on us and also the impact that the the weather situation or the transmission path has on us. And you see here that we can model the total one way transmission loss in the transmission medium where we have a local exemption coefficient. And we have a crossover function crossover function we will look at in a few slides and also the same for the local extinction coefficient. And obviously, we're integrating this over the whole distance. So the longer we're going through this weather effect that is reducing our signal, the more signal reduce and demand our range that we can

Unknown Speaker  24:15  
detect our target at is impacted. So typically, when you want to build such a sensor, you need a transmitting component and you need a receiving component where in the very beginning of this diagram, you have no overlap between the transmitting and receiving components. If the target would be here were marked the green area, nothing of the reflection would travel into the receiving component. However, it's after a certain point there is a full overlap of the two fields of use of the two components. And you can model this increasing return signal that you will get at certain distances as given above here for the configuration on the page Before you add the distances at which you have different amounts of overlap between the two, the transmitter and the receiver, and you can calculate this through simple geometry. Yeah, the expansions of that, feel free to implement it yourself if you want to. If you say, okay, where can I get the parameters of this, it's different from sensor sensor. If some of you actually go so far as to implement the model for file, then one thing you can do is you can use the curves that we had a few slides back where you saw this detection range over the different emission percentages. And you can then fit your curve to these two lighters to see that you get some realistic values out of it. Now the goal here is rather for you to understand all the different modules, so you can have different geometric configurations for your crossover functions and so on. But it's important that you know that that's such a phenomena exists. And now we are looking at the target. So here on the slides, and also feel free to check this out in some more depth in the paper, if you want to might be easier for you to understand even you have the plain surface of a solid object we were discussing, that doesn't change the time signature of the of the laser pulse, we're considering this a hard target, which means that if you send out a signal, which has a certain shape, you would assume that the signal that you get back has the same shape. Depending on how you hit the target. For example, if if it's a pickup truck, and a part of your your signal hits here, the dnainfo back and the part of it hits the deleter. And then the shape may actually somehow change. Now, we're talking about a simplification. So we considering how target where the signal that we get out, it's also how we get the signal back. And for this, we're not talking about differential reflectivity of a target. And there are different models how to model the way the target reflects. So here, simple number reflection model is suggested where you can also parameterize it. And you're supposed to know that these different models exist, and so on. So that if you need in the future, you can go look those specific models up, and, and then use them. So let's say you were in the situation, and you're not supposed to pick a reflectivity model for your target. There are many different ways of modelling this and they all have their own small mistakes. So this is just to guide you into what different kinds of approaches exist here.

Unknown Speaker  28:06  
When we talk about light being reflected from objects, we often talk about the bidirectional reflectance distribution function PDF, which is also well known and is relevant to any applications not just autonomous driving sensor models, but also like models for simulations for elimination of scenes and some many, many cases where this is relevant. So we're talking about the difference between an empirical model a theoretical model, an experimental model. And an empirical model provides a simple formulation to mimic a specific kind of reflection. To have low computational costs. They can can be adjusted with parameters without consideration of physics. So you don't ask is the model the equation that you create perfectly accurate with regards to how physics works, but you say that given us an input can you create an output with this model that usefully represents the phenomenon that you want to model. Instead, you can also come up with a more sophisticated theoretical model, where you accurately try to simulate all the different effects like the scattering of the light based on the physical laws. This often leads to very complex models or complex equations of high computational effort. But it's then of course, also based on strong physical basis. Or you can have an experimental model approach where you actually just go and you measure your PDF with the sensors that you need for that from with light sources in different positions, and so on. Of course, doing such an experiment takes a long time. So that's why it's so slow. And of course, your experiment also limits you if you have a theoretical model, you can then apply that to many situations. And if you do test you have a limited resolution that you're you're fitting your data to because you don't measure all possible configurations of slides that you can Have this just illustrates the point from last slide a little bit. So you see the DB RDF can be split in theoretical approaches into empirical approaches into experimental approaches, and that they then further differentiate themselves into the different groups. And then you have the actual different model implementations down here. So, depending on which use case you're pursuing your development, you will then choose one of these according to how much computational resources you can invest into it, you can consider how much accuracy you actually need, how much time you have to implement the model, and then you make a good selection. Next thing you want to do if you're looking at rain consequences on this, you want to

Unknown Speaker  30:58  
consider how does the specific type of rain that you have influenced your reduction and some models that exist? For example, consider consider the impact that different raindrop distributions have on the impact that the rain has on

Unknown Speaker  31:18  
your sensor detection range. So depending on what kind of droplet diameter you have, and how much of the droplet diameter you have in your terrain, the effect on the signal could be the different. And then you ask yourself, okay, what hours but how many droplets of which type Do we have. And you see that this is different for rain, that is the Continental area and rain that is in the coastal area. So you already see the complexity that some people driving in certain areas will never have an issue, while other people driving in other areas will have issues all the time. You see that we have an alpha here, and the better. The one, the first one, the alpha is the extinction coefficient. So here we're talking about how much signal Are you losing from the rain, but then there's another potential problem, you're also getting back scattering. So from the previous slides, you already know the back scattering means that now the light is being scattered back towards you. So it's actually returning before we even hit the target, you saw that these snow measurements, that the rain could theoretically also with the Android reflects back to us trigger detection on our sensor. So it's good to be aware of this. And then finally, when you put all of this together, you get an impression of how the weather effects can then impact your range. And here, for example, you see a comparison of measurements that were made in the recording simulation that was done for I believe in this case, it should probably be it's been fun, where focus is defined by you and visibility. The same human can see for 50 metres 400 metres for 200 metres and so on. And in this case, they were trying to create a calibrated LIDAR, where based on the detection distance it shows in a given situation, you can also say what kind of fog Do you have out there, which is a rare feature. Most sensors are not calibrated to the weather effects. So that you can tell from the ladder detection range, what type of work you have all day Exactly. We talked about this fog experiment some slides ago on the road with the dense fog that the car was passing. Now here you have an experiment with water that was conducted to test the simulation model that we checked out just out at BMW. And we already said that sometimes it can be challenged to create super realistic experiments. And he also they noticed that the problem that they had was that the sprinkler along in the roof where the water came out, they had concentration differences over time. So a different amount of water was present. Depending on how close lead these water sprinklers were aligned with each other. So you would maybe have areas of more density in your in your rain in some areas and other areas. It would be good for you to know what he needs. This means hardware in the loop. And it means that you're testing your software while also having the hardware in a test. Now how could such a system look for testing LIDAR? So you have to person computer here again from the cross functional remember that we need to transceiver and remitter so we're transmitting now adults and once the pulse hits this optical trigger, it can then give a signal back here where we can reconverted and so on and then output a signal into the receiver and which can modify the power of the signal that we output in order to pretend that the transmitter signal would have travelled for a certain distance. So we need to create a time delay, we need to change the power according to our expectations. And this whole thing needs to be calibrated, supplied with power and control here, also within Canada phase, and then theoretically, we could have a personal computer that connects here, in this case of zero pass into the microprocessor that then selects the scenario into scenarios. We could, for example, get from database, so you could drive around outside and you could measure real scenarios, how much delay was there? How big were our signals, how much noise was there, then we could recreate this in a table demonstrated. With that we have concluded this sensor topic part of lecture, which is very nice, so we can move on. And the goal of the next

Unknown Speaker  36:00  
part is for you to get a basic understanding of what are the expectations with regards to knowledge about new networks from you if you currently go into the autonomous driving domain. So this is not supposed to be like an in depth introduction to deep learning, because that takes much longer than just a few minutes, minutes, how long we're going to spend here. But it should point out to you a little bit as you're studying, if you want to go into AI and autonomous driving. What do people expect you to be familiar with? What modules be asked in application talks? So I expect that the majority of you are familiar with neural networks, so we can just very briefly touched us. And if you have specific questions, feel free to ask us, as well. So the neural networks have been introduced a very long time ago, 1943 here. And in the beginning, they were very promising, but not very powerful. Because the computer power was lacking, the result was lacking, the data was lacking in some of medical tricks were also lacking. So there was a lot of promise in the beginning. But not all of the promises were realised.

Unknown Speaker  37:17  
Because a lot was

Unknown Speaker  37:18  
promised and then not fulfilled. Ultimately, we ended up going into what's called what's called the AI winter. So there was a period of reduced funding and interest in artificial intelligence research. hype cycles, followed by disappointment or criticism, followed by funding cuts, followed by renewed interest years or decades later. Now over the last 10 years, we have had strong success in this domain. And already since many years, every year, you have breakthroughs in this area. And you always need to stay up to date with the architectures that are most successful. If you want to work in this domain. Something you will see in any deep learning machine learning vector, I guess, is the perceptron, a binary classifier to separate some objects according to their classification. So this is a very simple architecture. Typically, all these diagrams read from left to right, so you feed something into the left and you get an opera to the right.

Unknown Speaker  38:25  
can also add some more inputs, and you can have a layer in between. and you can call this a feed forward network. And you can use the activation functions in each of these neurons. And each of neurons represents a very simple equation. Again, if you have never heard about new networks, the debates for this you can can check in specialised lectures for this. So here we're just working our way to these architectural discussions. Now, we just added one layer on the last slide. Now we can also add more layers. And then if you have many layers in the middle, you start talking about deep learning, which means we have some depth of layers. Adding new layers or adding more depth often lead to improve performance, but it also leads to higher competition costs. It also leads to more difficulties if you want to formally mathematically verify and prove that certain network is safe or formally. Correct. And the bigger you make it you're also the more data you also need to get some additional changes as you make the networks bigger and bigger. Another important question when you choose your new network for the autonomous driving task is do you make a time dependent or not? Do you only use data from one frame or do you use data from multiple frames? Do you look at the last three camera images in order to say

Transcribed by https://otter.ai
